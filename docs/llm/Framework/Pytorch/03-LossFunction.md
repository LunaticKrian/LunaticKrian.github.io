# 损失计算

---

## 一、损失函数

神经网络的主要特点，就是可以从数据中进行“学习”。这个学习的过程，就是让训练数据自动决定最优的权重参数。

神经网络（深度学习）也是机器学习的一种；跟传统机器学习方法相比，神经网络不需要人工设置特征量（如SIFT、HOG等），这样就可以用同样的流程直接处理所有问题了。

### 1.常见损失函数

#### (1) MSE 均方误差

![img.png](03-LossFunction.assets/img.png)

---

#### (2) 交叉熵误差


---

### 2.分类任务损失函数

#### (1) 二分类任务损失函数 

二元交叉熵损失函数

---

#### (2) 多分类任务损失函数

多类交叉熵损失函数 

---

### 3.回归任务损失函数

#### (1) MAE 平均绝对误差

#### (2) MSE 均方误差

#### (3) Smooth L1

---

## 二、数值微分

### 1.导数和数值微分




### 2.偏导数



### 3.梯度


---

## 三、神经网络梯度计算

梯度计算：计算损失函数和W权重矩阵变化之间的关系



---

## 四、超参数

### 1.Epoch 训练轮次


### 2.Batch Size 批次大小


### 3.Iteration 迭代次数

---

## 五、梯度下降

### 1.梯度下降法

计算出每个权重的对于损失函数的偏导数（基于某一个点），更新权重：使用计算出的偏导数 * 学习率 -> 更新权重的值

### 2.随机梯度下降 SGD

在神经网络的学习过程中，可以使用梯度下降法来更新参数，目标就是减小损失函数的值。

实际操作时，一般会从训练数据中随机选择一个小批量数据（mini-batch），然后用梯度下降法迭代多个轮次（iteration）；这种“对随机选择的数据进行的梯度下降法”，被称作 随机梯度下降法（stochastic gradient descent，SGD）。



