# Netty Http 服务器请求阻塞

---

项目背景是我们团队在一个内部服务系统中落地了一个大模型的应用场景（下文中中表述为："对话生成场景"）。在技术选型阶段，我们考虑使用了MCP。
同时，这个场景在端到端的对话时延上有一定的要求，因此在请求模型后不能一直等待模型全部推理完成后在进行返回。
我们选择使用 SSE 进行流失输出，模型每返回一个 token 就通过 SSE 发送给上游服务，这样能有效降低在使用体验上的等待感。

最终，我们选择使用 Spring AI WebFlux 来实现这个场景。Spring AI WebFlux 是基于 Spring WebFlux 的一个扩展，专门用于处理 AI 模型的请求和响应。
Spring WebFlux 与 Spring MVC 不同，它是基于响应式编程模型的，能够更好地处理高并发和异步请求。背后依赖于 Netty 作为底层的 HTTP 服务器。

Spring AI：https://spring.io/projects/spring-ai#overview

![image-20250824124417317](01-NettyHttpServerRequestBlocking.assets/image-20250824124417317.png)

当初考虑到 Netty 的高性能和非阻塞特性，我们采用了异步回推的方式来处理模型的响应。
当上游服务发起请求时，首先将请进行封装放入内存队列中，然后立即返回一个响应，下游实时轮询内存队列异步处理请求（基于生产者消费者模式）。

起初在实验阶段，上游并发在10以内，服务稳定没有出现问题。后续又在这个模块服务上新增了其他场景的支持后，我们发现【对话生成场景】上游系统开始频繁报HTTP请求超时。

我们开始着手排查问题。

首先，我们确认上游服务的请求超时时间配置的3秒，我们的服务系统没有耗时操作，不至于请求后3秒内没有响应。
排除了网络抖动，服务宕机导致的请求超时（因为是部分请求超时，部分氢请求正常，且流量是均分到每一台机器上，每一台服务都有这样的情况）。

接着，我们根据上游服务系统提供的 request log id 进行日志检索，发现了一个有趣的现象：
这些请求中在我们服务系统中有些请求没有任何日志输出，说明这些请求根本没有到达我们的服务系统。有部分请求在发起请求后，延迟一分多钟才有日志输出。

这时我们怀疑是 Netty 服务器在处理请求时出现了问题。于是开始对 Netty 的配置进行排查。由于我们使用的是 Spring WebFlux 默认配置，没有对 底层的 Netty 进行任何优化。

初步认定是 Netty 的线程模型可能存在问题。Netty 使用了 Reactor 线程模型，分为 Boss 线程和 Worker 线程。 Boss 线程负责接收连接，Worker 线程负责处理 I/O 操作。
我们怀疑在高并发情况下，Worker 线程可能会被阻塞，导致无法及时处理新的请求。

> PS：这里为什么没有对服务的性能进行压测就上线呢？主要是因为当初在技术选型阶段，我们对 Spring WebFlux 和 Netty 的性能有一定的了解，认为其能够满足我们的需求。
> 
> 最重要的是：项目时间紧迫，我们没有足够的时间进行全面的性能测试。😭（倒排需求......）

接着，开始对 Netty 的线程模型进行配置。


Netty 默认是获取当前物理机的 CPU 核数 * 2作为 Worker 线程数。如果需要自定义配置的话需要重写 Netty 对于 Worker 线程数的配置。

```java

```

使用 Jmeter 进行压测，首先观察 Worker 线程数是否增加。